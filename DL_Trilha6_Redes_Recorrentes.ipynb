{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Trilha6_Redes_Recorrentes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marceloccs/DL-PARA-PLV/blob/main/DL_Trilha6_Redes_Recorrentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Nome:** Marcelo Custodio Cruz Silva\n",
        "\n",
        "##**Matricula:** 92123341\n",
        "\n",
        "---\n",
        "\n",
        "##**Curso:** Inteligencia Artificial\n",
        "\n",
        "##**Turma:** B\n",
        "\n",
        "*Link do arquivo no google colabs: https://drive.google.com/file/d/1nIR3WfiO2FcawdMTV6b69N0pclEWJQtO/view?usp=sharing*\n",
        "\n",
        "*Link do arquivo no github: https://github.com/marceloccs/DL-PARA-PLV/blob/main/DL_Trilha6_Redes_Recorrentes.ipynb* \n",
        "\n"
      ],
      "metadata": {
        "id": "dCW5dNILJR0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta tarefa, você deverá desenvolver um categorizador de notícias utilizando redes neurais recorrentes. Para isso, utilize o conjunto de dados Reuters presente nativamente no TensorFlow-Keras, *podendo carregá-lo desta forma (sugestão)*: \n",
        "\n",
        "```\n",
        "from keras.datasets import reuters \n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=10000)\n",
        "```\n",
        "\n",
        "Queremos avaliar a sua criatividade na formatação e solução do problema. Sendo assim, você poderá decidir sobre as técnicas e pré-processamento, os algoritmos **(SimpleRNN ou LSTM)** e a quantidade de **classes (2 ou 46)** que mais achar apropriado. Ao final, **apresente a acurácia do modelo**, buscando resultados superiores a **80% de acertos**. "
      ],
      "metadata": {
        "id": "ttdXZoBQJZc6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K-ksX3sAHf8"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFrgKXHgALlB"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.datasets import reuters"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Wk2ETPA7lf"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import SpatialDropout1D, Dropout, Dense, Embedding, SimpleRNN\n",
        "from tensorflow.keras import models, regularizers, layers, optimizers, losses, metrics"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Carregar os dados"
      ],
      "metadata": {
        "id": "tb2k0w4zQFeV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9B6FJAAQlj"
      },
      "source": [
        "vocab = 1000\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=vocab)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsAp_5_wGglK"
      },
      "source": [
        "word_map = reuters.get_word_index()"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pré-processar os dados de entrada"
      ],
      "metadata": {
        "id": "y2hH85u-QOg7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cErw-4AAQu4"
      },
      "source": [
        "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=200)\n",
        "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=200)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Construir o modelo"
      ],
      "metadata": {
        "id": "bwgUlc5dQRWG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuXbkVbRAQ44"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(vocab, output_dim=46))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(np.unique(y_test).shape[0], activation='softmax'))"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Compilar o modelo"
      ],
      "metadata": {
        "id": "t1htwh3CQUIz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0v8y6aSAaEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fa4168-0c3f-4502-80eb-7b0610878e02"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, None, 46)          46000     \n",
            "                                                                 \n",
            " spatial_dropout1d_7 (Spatia  (None, None, 46)         0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 32)                10112     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 46)                1518      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57,630\n",
            "Trainable params: 57,630\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Ajustar o modelos aos dados de treinamento"
      ],
      "metadata": {
        "id": "YgrXJpwoQYdD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwPi8fbCAh0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eedff337-f9c2-4dae-8e57-f37e8b52ae92"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=52, epochs=150)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "173/173 [==============================] - 18s 94ms/step - loss: 2.4642 - accuracy: 0.3861\n",
            "Epoch 2/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.9801 - accuracy: 0.4762\n",
            "Epoch 3/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.8465 - accuracy: 0.5121\n",
            "Epoch 4/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 1.7746 - accuracy: 0.5334\n",
            "Epoch 5/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.7104 - accuracy: 0.5538\n",
            "Epoch 6/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 1.6714 - accuracy: 0.5705\n",
            "Epoch 7/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.6460 - accuracy: 0.5753\n",
            "Epoch 8/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.6020 - accuracy: 0.5826\n",
            "Epoch 9/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.5633 - accuracy: 0.5907\n",
            "Epoch 10/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.5492 - accuracy: 0.5946\n",
            "Epoch 11/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.5055 - accuracy: 0.6108\n",
            "Epoch 12/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.4689 - accuracy: 0.6227\n",
            "Epoch 13/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 1.4327 - accuracy: 0.6366\n",
            "Epoch 14/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.4165 - accuracy: 0.6389\n",
            "Epoch 15/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.3900 - accuracy: 0.6485\n",
            "Epoch 16/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.3655 - accuracy: 0.6554\n",
            "Epoch 17/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.3364 - accuracy: 0.6628\n",
            "Epoch 18/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.3103 - accuracy: 0.6708\n",
            "Epoch 19/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.2937 - accuracy: 0.6777\n",
            "Epoch 20/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.2685 - accuracy: 0.6874\n",
            "Epoch 21/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.2526 - accuracy: 0.6900\n",
            "Epoch 22/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.2294 - accuracy: 0.6966\n",
            "Epoch 23/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.2164 - accuracy: 0.7004\n",
            "Epoch 24/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 1.1956 - accuracy: 0.7076\n",
            "Epoch 25/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 1.1886 - accuracy: 0.7108\n",
            "Epoch 26/150\n",
            "173/173 [==============================] - 16s 91ms/step - loss: 1.1784 - accuracy: 0.7149\n",
            "Epoch 27/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.1547 - accuracy: 0.7194\n",
            "Epoch 28/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 1.1411 - accuracy: 0.7250\n",
            "Epoch 29/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.1446 - accuracy: 0.7238\n",
            "Epoch 30/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.1342 - accuracy: 0.7288\n",
            "Epoch 31/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.1036 - accuracy: 0.7363\n",
            "Epoch 32/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 1.0944 - accuracy: 0.7360\n",
            "Epoch 33/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.0931 - accuracy: 0.7341\n",
            "Epoch 34/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.0818 - accuracy: 0.7379\n",
            "Epoch 35/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.0715 - accuracy: 0.7413\n",
            "Epoch 36/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.0716 - accuracy: 0.7485\n",
            "Epoch 37/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.0487 - accuracy: 0.7465\n",
            "Epoch 38/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 1.0622 - accuracy: 0.7508\n",
            "Epoch 39/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.0400 - accuracy: 0.7531\n",
            "Epoch 40/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.0344 - accuracy: 0.7522\n",
            "Epoch 41/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.0331 - accuracy: 0.7530\n",
            "Epoch 42/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 1.0272 - accuracy: 0.7521\n",
            "Epoch 43/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 1.0196 - accuracy: 0.7595\n",
            "Epoch 44/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.0107 - accuracy: 0.7625\n",
            "Epoch 45/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 1.0052 - accuracy: 0.7605\n",
            "Epoch 46/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.9869 - accuracy: 0.7655\n",
            "Epoch 47/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9974 - accuracy: 0.7651\n",
            "Epoch 48/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9937 - accuracy: 0.7685\n",
            "Epoch 49/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.9720 - accuracy: 0.7693\n",
            "Epoch 50/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.9775 - accuracy: 0.7710\n",
            "Epoch 51/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9625 - accuracy: 0.7743\n",
            "Epoch 52/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9569 - accuracy: 0.7731\n",
            "Epoch 53/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9564 - accuracy: 0.7762\n",
            "Epoch 54/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9436 - accuracy: 0.7772\n",
            "Epoch 55/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9472 - accuracy: 0.7774\n",
            "Epoch 56/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.9307 - accuracy: 0.7808\n",
            "Epoch 57/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9298 - accuracy: 0.7782\n",
            "Epoch 58/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.9261 - accuracy: 0.7817\n",
            "Epoch 59/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.9241 - accuracy: 0.7808\n",
            "Epoch 60/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9096 - accuracy: 0.7867\n",
            "Epoch 61/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9081 - accuracy: 0.7888\n",
            "Epoch 62/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8979 - accuracy: 0.7923\n",
            "Epoch 63/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.9032 - accuracy: 0.7900\n",
            "Epoch 64/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.9120 - accuracy: 0.7890\n",
            "Epoch 65/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8944 - accuracy: 0.7907\n",
            "Epoch 66/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8903 - accuracy: 0.7945\n",
            "Epoch 67/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8838 - accuracy: 0.7946\n",
            "Epoch 68/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.8887 - accuracy: 0.7933\n",
            "Epoch 69/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8875 - accuracy: 0.7930\n",
            "Epoch 70/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8794 - accuracy: 0.7966\n",
            "Epoch 71/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8694 - accuracy: 0.7983\n",
            "Epoch 72/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8721 - accuracy: 0.7975\n",
            "Epoch 73/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8661 - accuracy: 0.7988\n",
            "Epoch 74/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8629 - accuracy: 0.7999\n",
            "Epoch 75/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8603 - accuracy: 0.8000\n",
            "Epoch 76/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8594 - accuracy: 0.7971\n",
            "Epoch 77/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8453 - accuracy: 0.8014\n",
            "Epoch 78/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8449 - accuracy: 0.8027\n",
            "Epoch 79/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.8490 - accuracy: 0.8008\n",
            "Epoch 80/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8453 - accuracy: 0.8035\n",
            "Epoch 81/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.8344 - accuracy: 0.8079\n",
            "Epoch 82/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8353 - accuracy: 0.8071\n",
            "Epoch 83/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8368 - accuracy: 0.8066\n",
            "Epoch 84/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8330 - accuracy: 0.8033\n",
            "Epoch 85/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8291 - accuracy: 0.8078\n",
            "Epoch 86/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8242 - accuracy: 0.8082\n",
            "Epoch 87/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8120 - accuracy: 0.8107\n",
            "Epoch 88/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8138 - accuracy: 0.8085\n",
            "Epoch 89/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.8216 - accuracy: 0.8102\n",
            "Epoch 90/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8110 - accuracy: 0.8095\n",
            "Epoch 91/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8171 - accuracy: 0.8081\n",
            "Epoch 92/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8145 - accuracy: 0.8103\n",
            "Epoch 93/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8069 - accuracy: 0.8093\n",
            "Epoch 94/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8115 - accuracy: 0.8097\n",
            "Epoch 95/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.8041 - accuracy: 0.8112\n",
            "Epoch 96/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7881 - accuracy: 0.8150\n",
            "Epoch 97/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7990 - accuracy: 0.8121\n",
            "Epoch 98/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7950 - accuracy: 0.8136\n",
            "Epoch 99/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7898 - accuracy: 0.8169\n",
            "Epoch 100/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7858 - accuracy: 0.8167\n",
            "Epoch 101/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7925 - accuracy: 0.8157\n",
            "Epoch 102/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7827 - accuracy: 0.8152\n",
            "Epoch 103/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7839 - accuracy: 0.8164\n",
            "Epoch 104/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7810 - accuracy: 0.8174\n",
            "Epoch 105/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7751 - accuracy: 0.8190\n",
            "Epoch 106/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7736 - accuracy: 0.8194\n",
            "Epoch 107/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7786 - accuracy: 0.8200\n",
            "Epoch 108/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7568 - accuracy: 0.8228\n",
            "Epoch 109/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7724 - accuracy: 0.8202\n",
            "Epoch 110/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7671 - accuracy: 0.8223\n",
            "Epoch 111/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7689 - accuracy: 0.8230\n",
            "Epoch 112/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7700 - accuracy: 0.8206\n",
            "Epoch 113/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7637 - accuracy: 0.8200\n",
            "Epoch 114/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7519 - accuracy: 0.8249\n",
            "Epoch 115/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7612 - accuracy: 0.8242\n",
            "Epoch 116/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7741 - accuracy: 0.8202\n",
            "Epoch 117/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7485 - accuracy: 0.8251\n",
            "Epoch 118/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7588 - accuracy: 0.8212\n",
            "Epoch 119/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7578 - accuracy: 0.8254\n",
            "Epoch 120/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7433 - accuracy: 0.8262\n",
            "Epoch 121/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7602 - accuracy: 0.8239\n",
            "Epoch 122/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7516 - accuracy: 0.8238\n",
            "Epoch 123/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7539 - accuracy: 0.8234\n",
            "Epoch 124/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7476 - accuracy: 0.8231\n",
            "Epoch 125/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7353 - accuracy: 0.8295\n",
            "Epoch 126/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7595 - accuracy: 0.8234\n",
            "Epoch 127/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7385 - accuracy: 0.8274\n",
            "Epoch 128/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7191 - accuracy: 0.8304\n",
            "Epoch 129/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7433 - accuracy: 0.8280\n",
            "Epoch 130/150\n",
            "173/173 [==============================] - 16s 92ms/step - loss: 0.7213 - accuracy: 0.8288\n",
            "Epoch 131/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7359 - accuracy: 0.8307\n",
            "Epoch 132/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7442 - accuracy: 0.8257\n",
            "Epoch 133/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7287 - accuracy: 0.8299\n",
            "Epoch 134/150\n",
            "173/173 [==============================] - 17s 96ms/step - loss: 0.7232 - accuracy: 0.8323\n",
            "Epoch 135/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7263 - accuracy: 0.8314\n",
            "Epoch 136/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7335 - accuracy: 0.8320\n",
            "Epoch 137/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7327 - accuracy: 0.8291\n",
            "Epoch 138/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7267 - accuracy: 0.8302\n",
            "Epoch 139/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7404 - accuracy: 0.8304\n",
            "Epoch 140/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7090 - accuracy: 0.8343\n",
            "Epoch 141/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7155 - accuracy: 0.8353\n",
            "Epoch 142/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7139 - accuracy: 0.8317\n",
            "Epoch 143/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7097 - accuracy: 0.8357\n",
            "Epoch 144/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7178 - accuracy: 0.8343\n",
            "Epoch 145/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7212 - accuracy: 0.8346\n",
            "Epoch 146/150\n",
            "173/173 [==============================] - 16s 93ms/step - loss: 0.7096 - accuracy: 0.8312\n",
            "Epoch 147/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7127 - accuracy: 0.8333\n",
            "Epoch 148/150\n",
            "173/173 [==============================] - 16s 94ms/step - loss: 0.7026 - accuracy: 0.8336\n",
            "Epoch 149/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7062 - accuracy: 0.8373\n",
            "Epoch 150/150\n",
            "173/173 [==============================] - 16s 95ms/step - loss: 0.7013 - accuracy: 0.8375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5fe27b8150>"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Avaliar o modelo nos dados de teste"
      ],
      "metadata": {
        "id": "7fYk9eiRQbrs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trvbnpRGAisR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf998c7-6514-4c55-b57b-89a3debc83d9"
      },
      "source": [
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print('Acurancia do modelo durtante os testes: {:.2f}%'.format(accuracy_score(y_test, np.argmax(y_pred, axis=1))*100))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurancia do modelo durtante os testes: 72.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusão"
      ],
      "metadata": {
        "id": "zcFYj6EZQesB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mesmo com aumentando a épocas ou o número de tensores e classes, o LTSM não apresentou bons resultados acima de 80% de acurácia durante os testes. Realizei testes evitando utilizar o modelo do LSTM, apresentou uma melhor performance, foi mais rápido o treinamento e com uma acurácia melhor, utilizando apenas  2 camadas de redes “relu” com Dropout e saída de dados com softmax.\n",
        "\n",
        "Logo o LSTM é muito bom e apresenta um treinamento inicial com grande vantagem, porém não apresentou uma boa performance no dataset apresentado, pode ser que o tratamento de tokenização ou melhor tratamento do mesmo melhore o resultado apresentado.\n"
      ],
      "metadata": {
        "id": "X9LLY2m3jmTR"
      }
    }
  ]
}